{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9189e8ee",
   "metadata": {},
   "source": [
    "# Step 0\n",
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4969f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracing information from pdf\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# splitting & embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# vector db\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# import answer generation \n",
    "import threading\n",
    "import subprocess\n",
    "\n",
    "# allow me to execute bash commands\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b62708",
   "metadata": {},
   "source": [
    "## Step 1: \n",
    "### Extract text from text books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ee531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file:  5G_mobile_communication_concepts.pdf\n",
      "Reading file:  5G_wireless.pdf\n",
      "Converting file:  content/5G_mobile_communication_concepts.pdf\n",
      "Converting file:  content/5G_wireless.pdf\n",
      "The total page of the textbooks are :  939\n",
      "The total number of words are :  2415197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pdf_folder_path = \"content/\"\n",
    "\n",
    "loaders = []\n",
    "for file in os.listdir(pdf_folder_path):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        print(\"Reading file: \", file)\n",
    "        loaders.append(PyPDFLoader(os.path.join(pdf_folder_path, file)))\n",
    "\n",
    "def load_pdf(loaders):\n",
    "    full_documents = []\n",
    "    for loader in loaders:\n",
    "        print(\"Converting file: \", loader.file_path)\n",
    "        documents = loader.load()\n",
    "        full_documents.extend(documents)\n",
    "    return full_documents\n",
    "\n",
    "def convert_to_text(documents):\n",
    "    full_text = \"\"\n",
    "    for document in documents:\n",
    "        if len(document.page_content) > 20:\n",
    "            full_text += document.page_content\n",
    "    return full_text\n",
    "    \n",
    "\n",
    "\n",
    "full_documents = load_pdf(loaders)\n",
    "print(\"The total page of the textbooks are : \", len(full_documents))\n",
    "full_text = convert_to_text(full_documents)\n",
    "print(\"The total number of words are : \", len(full_text))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89243286",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "## Preprocessing, clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c963908",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cleaned_lines)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cleaned_text\n\u001b[1;32m---> 16\u001b[0m clean_texted_text \u001b[38;5;241m=\u001b[39m clean_text(\u001b[43mfull_text\u001b[49m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total number of words after cleaning are : \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(clean_texted_text))\n\u001b[0;32m     19\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcleaned_text.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'full_text' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_lines = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    for line in lines:\n",
    "        line = remove_extra_spaces(line)\n",
    "        cleaned_lines.append(line)\n",
    "    cleaned_text = \"\\n\".join(cleaned_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "clean_texted_text = clean_text(full_text)\n",
    "print(\"The total number of words after cleaning are : \", len(clean_texted_text))\n",
    "\n",
    "file_name = \"cleaned_text.txt\"\n",
    "with open(file_name, 'w', encoding='utf-8') as f:\n",
    "    f.write(clean_texted_text)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e9909",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "### Splitting texts into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c72ee17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of chunks are :  6061\n",
      "The vector db is saved in the faiss_index folder\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "with open('cleaned_text.txt', 'r', encoding='utf-8') as f:\n",
    "    clean_texted_text = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(clean_texted_text)\n",
    "print(\"The total number of chunks are : \", len(chunks))\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vector_db = FAISS.from_texts(\n",
    "    texts = [chunk for chunk in chunks],\n",
    "    embedding = embeddings,\n",
    ")\n",
    "\n",
    "vector_db.save_local(\"faiss_index\")\n",
    "print(\"The vector db is saved in the faiss_index folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1710c",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "### Query Processing & Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e7e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_context(query, k=3, score_threshold=0.8):\n",
    "    retrieved_context = vector_db.similarity_search(\n",
    "        query,\n",
    "        k = k,\n",
    "        score_threshold = score_threshold\n",
    "    )\n",
    "    return retrieved_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33fe468",
   "metadata": {},
   "source": [
    "## Step 5:\n",
    "### Generting Answer with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59036cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is :  According to the provided context, 4G systems \"enable mobile broadband in the true sense, targeting 100 Mbps or higher on the move.\" In other words, 4G systems provide high-speed data services that allow for fast internet access while moving.\n",
      "\n",
      ", type 'quit' or 'exit' to stop the program\n",
      "The answer is :  Based on the provided context, I can confidently answer that:\n",
      "\n",
      "**5G refers to the fifth generation of mobile communications, which is expected to support a new radio access technology called 5G-NR (new radio) and an enhanced core network called NGC (Next Generation Core).**\n",
      "\n",
      "In addition to this specific definition, the text also highlights that 5G can be used in a wider context to refer to a wide range of new services envisioned to be enabled by future mobile communication.\n",
      "\n",
      ", type 'quit' or 'exit' to stop the program\n",
      "The answer is :  No relevant context found.\n",
      "\n",
      ", type 'quit' or 'exit' to stop the program\n",
      "The answer is :  No relevant context found.\n",
      "\n",
      ", type 'quit' or 'exit' to stop the program\n",
      "The answer is :  No relevant context found.\n",
      "\n",
      ", type 'quit' or 'exit' to stop the program\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "llm = OllamaLLM(model='llama3')\n",
    "\n",
    "def answer_question(question, context, llm):\n",
    "    # Reformat context\n",
    "    formatted_context = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "    # Prompt Template\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert research assistant specializing in answering questions about research papers.\n",
    "\n",
    "    Task: Answer the question based on the provided context, with detail explaination and reasoning.\n",
    "\n",
    "    Instructions:\n",
    "    * Be concise and accurate.\n",
    "    * If the context does not contain the answer, say EXACTLY \"I cannot answer confidently\"\n",
    "    * If the question is unrelated to the context, say EXACTLY \"NA\"\n",
    "    * If the question asks for a yes/no answer, provide it and explain your reasoning shortly.\n",
    "\n",
    "    Context:\n",
    "    {formatted_context}\n",
    "\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate answer using the LLM\n",
    "    try:\n",
    "        response = llm.invoke(prompt)  # Use the llm object directly\n",
    "        return response.strip() # Remove leading/trailing whitespace\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM call: {e}\")\n",
    "        return \"Error processing the request.\"\n",
    "\n",
    "def response(query, k=10, score_threshold=0.8):\n",
    "    retrieved_context = retrive_context(query, k=k, score_threshold=score_threshold)\n",
    "    if not retrieved_context:\n",
    "        return \"No relevant context found.\"\n",
    "    \n",
    "    # Answer the question using the LLM\n",
    "    response = answer_question(query, retrieved_context, llm)\n",
    "    return response\n",
    "\n",
    "\n",
    "while True:\n",
    "    query = input(\"Enter your question:\")\n",
    "    if query.upper() == (\"QUIT\") or query.upper() == (\"EXIT\"):\n",
    "        print(\"Exiting the program\")\n",
    "        break\n",
    "    if query == \"\":\n",
    "        print(\"Please enter a valid question\")\n",
    "        continue\n",
    "    answer = response(query, k=10, score_threshold=0.8)\n",
    "    print(\"The answer is : \", answer)\n",
    "    print(\"\\n\", \"type 'quit' or 'exit' to stop the program\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
